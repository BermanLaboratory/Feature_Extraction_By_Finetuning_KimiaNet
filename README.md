Feature Extractor
==============================

Finetuning DenseNet121 architechture using weights of the model provided by KimiaNet for extracting features From Whole Slide Image Patches relevant to Cancer Grades

## Updates:

## Pre-requisites:
    * Linux
    * 

## Testing And Evaluation Script

For training the model on custom dataset
```shell

python train --config CONFIG_FILE_PATH --mode train

```

For Testing the model of custom model using pretrained weights:

```shell

python train --config CONFIG_FILE_PATH --mode test
```

Additional flags that can be passed :
* `--config`: configuration file path
* `--mode` : train or test mode
* `--gpus` : gpus to use (default : [2]). Use nvidia-smi to check for free gpus.
* `--batch_size_train` : batch_size for training the model
* `--batch_size_test`
: batch_size for testing the model

## Running External Cohorts on This Code

## Preprocessing Image Dataset (.vsi Images) :

### Creating Image Tiles/Patches Using QuPath:
1. Open QuPath -> File -> Project -> Create Project -> New Folder -> Open
2. Add Images -> Choose Files -> Select Images to Add to Project (only the .vsi files)
3. Go to search entry in project at the bottom -> type 'overview' -> Right Click on Test_Project -> Remove Images
4. Select Any Image -> Workflow -> Create Script -> File -> Open  ( Select The Script For Tiling ).

QuPath Script Link : [a relative link](/src/data/data_preprocessing/Tile_Exporter.groovy)
The Script is also Present in the Backup Drive to Run it Directly on the dataset.

Tiling Parameters in the Script That Can be Changed: 
`downsample`: This parameters helps to select the resolution we want to work with. By default set to 1. Means max resolution of the file that is 20X. The final resoultion of image will be `orginial max resolution / downsample`
`imageExtension`: '.png' is default . It is a lossless compression format. '.tiff' is good but it uses lot more memory for storage.
`tileSize`: specify the size of tiles to use. (default: 1000)
`overlap`: how much do you want tiles to overlap (default: 0)
`outputPath`: Specify the output path for the tiles to be stored.


The Script Can be Run for Both a Single WSI or the Complete Project:
Right Click on the Script Editor of Qupath: * Choose 'Run' For A single WSI
                                            * Choose 'Run For Project' For Running the Script for the complete project


### Remove Tiles That Have No info in them.
This is done using the file size of the created image patches. The python script will run directly by just using the dataset folder and file size using command line.
Default value for file size for 1000 pixel .png images is : 1 mb.
The File Size can be determined by sorting the patches based on file size using file explorer and then determine the appropriate threshold.

Remove Tiles Python Script : [a relative link](/data/Code/kimianet_feature_extractor/src/data/data_preprocessing/remove_empty_tiles.py)

Sample Code For running the Script:
`python remove_empty_tiles.py TILED_DATASET_PATH `

Optional Arguments:
`--file_size` : File Size Threshold ( Default: 1)

### Getting The Image Tiles/Patches to the server

scp -r 'local dataset folder path' USER@SERVER_IP:'server directory folder path'

### Stain Normalization And Color Augmetation
Stain Normalization Can Either be Done dynamically when loading images to the deep learning model or creating by creating a completely new dataset.

* For Creating a New Dataset After Stain Normalization
REQUIREMENTS:
 * StainTools, HistomicsTK

* For Dynamically Loading Images That are Stain Normalized Into the model

### Patch Clustering Based on DenseNet Features

#### Feature Extraction

#### Clustering

### Patch/Tile Score Calculation 
Score Can be calculated by just taking into consideration the number of nuclei or by using the histolab's implementation of tissue ratio plus the nuclei ratio.

Tile Scorer Script : [a relative link](/kimianet_feature_extractor/src/data/data_preprocessing/tile_scorer.py)

Arguments for the script:
`src` : src dataset folder
`dst` : destination folder for storing the csv files generated by the script
`type` : Type of scorer to use (default : nuclei_and_tissue) can be just nuclei as well

Sample Command For Running the Script:
`python tile_scorer.py DATASET_PATH DESTINATION_PATH --type nuclei_and_tissue`

## Feature Visualization And Clustering

## Trained Model Checkpoints

## Examples

Project Organization
------------

    ├── LICENSE
    ├── README.md          <- The top-level README for developers using this project.
    ├── data
    │   ├── final dataset  <- Tiles Selected as final dataset after Nuclei Ratio calculation and Removal of Artifact Tiles
    │   ├── interim        <- Intermediate data that has been transformed. Empty Tiles Removed and Stain Normalization Done
    │   ├── Tiled_Dataset  <- The final, canonical data sets for modeling.
    │   └── raw            <- The original, immutable data dump.
    │
    ├── docs               <- A default Sphinx project; see sphinx-doc.org for details
    │
    ├── models             <- Trained and serialized models, model predictions, or model summaries
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │                         the creator's initials, and a short `-` delimited description, e.g.
    │                         `1.0-pk-initial-data-exploration`.
    │
    ├── references         <- Data dictionaries, manuals, and all other explanatory materials.
    │
    ├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with pipreqs
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    └── src                <- Source code for use in this project.
        ├── __init__.py    <- Makes src a Python module
        │
        |
        ├── config
        |    └──bermanlab.yaml  <- configurations for training and testing the model
        |
        ├── data           <- Scripts to transform data , dataloaders , dataset classes
        │   └── make_dataset.py
        │
        ├── features       <- Scripts to turn raw data into features for modeling
        │   └── build_features.py
        │
        ├── models    <- Scripts to train models and then use trained models to make predictions
        │   ├── architechture
        |   ├── model architechture
        |   |     └──kimianet_modified.py   <- Pytorch Lightning module for the model
        │   ├── predict_model.py
        │   └── train_model.py
        │
        └── visualization  <- Scripts to create Visual Dictionary Using Extracted Features After fine Tuning the Model
               └── visualize.py

            

--------


## Issues:
- All issues reported on the forum of the repository
