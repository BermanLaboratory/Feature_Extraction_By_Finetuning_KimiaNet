{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required Libraries\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# The Documentation For Facebook's Faiss Library for PCA, clustering etc can be found at \n",
    "# https://github.com/facebookresearch/faiss/wiki/Faiss-building-blocks:-clustering,-PCA,-quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_path = '/mnt/largedrive0/katariap/feature_extraction/data/Dataset/DenseNet_Features'\n",
    "feature_vector_folder = '/mnt/largedrive0/katariap/feature_extraction/data/Dataset/DenseNet_Features' #Path to Folder with Extracted feature Vectors\n",
    "densenet_features_files = glob(feature_vector_folder+'/*.json') #Extract the file List in the feature_vector_folder using Glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a List of Features Values for all WSIs\n",
    "\n",
    "feature_values = []\n",
    "file_names = []\n",
    "\n",
    "for feature_file in densenet_features_files:\n",
    "    #Json loads is used to read each feature vector file\n",
    "    with open(feature_file,\"r\") as file:\n",
    "        feature_dictionary = json.loads(file.read())\n",
    "    \n",
    "    feature_values = feature_values + [np.array(list(feature_dictionary.values()))]\n",
    "    file_names = file_names + [np.array(list(feature_dictionary.keys()))]\n",
    "\n",
    "\n",
    "#Estimated Run Time is 3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "file_list = []\n",
    "i = 0 #Counter helps to keep track of files processed.\n",
    "\n",
    "# Create A List with feature values corresponding to each Image Patch\n",
    "for folder in feature_values:\n",
    "    for file in folder:\n",
    "        feature_list = feature_list + [file]\n",
    "    i = i + 1\n",
    "    print(i)\n",
    "\n",
    "#Estimated Run Time is 7 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final List of all Image Patch Names (Each Name contains the Patch Path)\n",
    "i = 0\n",
    "for folder in file_names:\n",
    "    for file in folder:\n",
    "        file_list = file_list + [file]\n",
    "    i = i +1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Facebook Faiss for PCA. Reducing 1024 Feature Vectors to 500\n",
    "# Faiss only uses 'float32' type arrays\n",
    "\n",
    "mat = faiss.PCAMatrix (1024, 500)\n",
    "mat.train(np.array(feature_list).astype('float32'))\n",
    "assert mat.is_trained\n",
    "feature_values_transformed = mat.apply(np.array(feature_list).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408215\n",
      "408215\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check for the count\n",
    "print(len(file_list))\n",
    "print(len(feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling a subset of 5120 / 408215 for training\n",
      "Clustering 5120 points in 500D to 20 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.11 s\n",
      "  Iteration 19 (2.09 s, search 1.52 s): objective=788919 imbalance=1.070 nsplit=0           \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "788919.125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of clusters to form on the basis of Features. I Found a number between 15 to 20 to be ideal to remove unwanted Image Patches\n",
    "ncentroids = 20\n",
    "niter = 20\n",
    "verbose = True\n",
    "kmeans = faiss.Kmeans(feature_values_transformed.shape[1], ncentroids, niter=niter, verbose=verbose)\n",
    "kmeans.train(feature_values_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = kmeans.index.search(feature_values_transformed, 1)\n",
    "# Mapping To each Cluster Centroid for all patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = feature_values_transformed.shape[1]\n",
    "index = faiss.IndexFlatL2 (d)\n",
    "index.add (feature_values_transformed)\n",
    "D_c, I_c = index.search (kmeans.centroids, 20) # To find 20 Representative Patches corresponding to Each Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Clustering Results to a CSV File, To avoid recalculation in case jupyter notebook fails.\n",
    "data_frame = pd.DataFrame(file_list,columns = ['filename'])\n",
    "data_frame['Cluster'] = I\n",
    "data_frame['Distance'] = D\n",
    "data_frame.to_csv('/mnt/largedrive0/katariap/feature_extraction/data/Dataset/Clusters_densenet.csv') #Change Path According to Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Lists Cluster Wise\n",
    "# Each Cluster Index will contain the files corresponding to it\n",
    "clusters = {}\n",
    "for i in range(len(file_list)):\n",
    "    if (I[i] not in list(clusters.keys())):\n",
    "        \n",
    "        clusters[I[i][0]] = [file_list[i]]\n",
    "    else:\n",
    "        clusters[I[i][0]] = clusters[I[i][0]] + [file_list[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the Clusters List as a pickle file.\n",
    "with open('/mnt/largedrive0/katariap/feature_extraction/data/Dataset/clusters.pickle', 'wb') as file:\n",
    "    pickle.dump(clusters, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run This cell in case loading of clusters is required from a file\n",
    "clusters = {}\n",
    "cluster_file = '/mnt/largedrive0/katariap/feature_extraction/data/Dataset/clusters.pickle'\n",
    "with open(cluster_file,'rb') as data_file:\n",
    "    clusters = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster Visualization. Visualize the Clusters By Running This Cell. \n",
    "for number in range(ncentroids):\n",
    "\n",
    "        fig = plt.figure(figsize = (30,30))\n",
    "        files = clusters[number]\n",
    "\n",
    "        if len(files) > 10:\n",
    "            files = random.sample(files,10) # 10 Random Files are selected from each cluster. Each Run display Different Files\n",
    "        for index,file in enumerate(files):\n",
    "            plt.subplot(5,5,index+1)\n",
    "            name = file.split('/')[-1]\n",
    "            img = Image.open(file)\n",
    "            img = np.array(img)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(name ,fontsize = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "selected_clusters = [1,2,4,6,7,8,9,10,11,12,13,17,18,19]  #Select The Clusters For Creating Final Dataset. Cluster Numbers START from 0.\n",
    "for i in selected_clusters:\n",
    "    final_list = final_list + clusters[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The selected Patches are saved as CSV File.\n",
    "selected_patches = pd.DataFrame(final_list, columns = ['Patch'])\n",
    "selected_patches.to_csv('/mnt/largedrive0/katariap/feature_extraction/data/Dataset/selected_after_clustering.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('feature_extraction': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "691d245293dbad75c885b8bfbee360e473ff8124e9f687a815643771a08ab899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
